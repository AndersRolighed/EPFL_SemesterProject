{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3da8e7f9",
   "metadata": {},
   "source": [
    "### FedAvg algorithm, currently working for both Logistic Regression and 1 hidden layer NN.\n",
    "##### See bottom for specific calls\n",
    "https://towardsdatascience.com/logistic-regression-with-pytorch-3c8bbea594be\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0656c44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i DataCorruption.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "209b458c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509b7c59",
   "metadata": {},
   "source": [
    "## MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2549b91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, model_params):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "        input_dim = model_params[\"input_dim\"]\n",
    "        hidden_dim = model_params[\"hidden_dim\"]\n",
    "        output_dim = model_params[\"output_dim\"]\n",
    "        \n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "4fca4102",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_MNIST(nn.Module):\n",
    "    def __init__(self, model_params):\n",
    "        super(Net_MNIST, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 5)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 5)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(32 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "            x = self.pool1(torch.relu(self.conv1(x)))\n",
    "            x = self.pool2(torch.relu(self.conv2(x)))\n",
    "            x = x.view(-1, 32 * 4 * 4)\n",
    "            x = torch.relu(self.fc1(x))\n",
    "            x = torch.relu(self.fc2(x))\n",
    "            x = self.fc3(x)\n",
    "            return x\n",
    "        \n",
    "        \n",
    "class Net_CIFAR10(nn.Module):\n",
    "    def __init__(self, model_params):\n",
    "        super(Net_CIFAR10, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 5)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 5)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(32 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "            x = self.pool1(torch.relu(self.conv1(x)))\n",
    "            x = self.pool2(torch.relu(self.conv2(x)))\n",
    "            x = x.view(-1, 32 * 5 * 5)\n",
    "            x = torch.relu(self.fc1(x))\n",
    "            x = torch.relu(self.fc2(x))\n",
    "            x = self.fc3(x)\n",
    "            return x\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def55269",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, model_params):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        \n",
    "        input_dim = model_params[\"input_dim\"]\n",
    "        output_dim = model_params[\"output_dim\"]\n",
    "        \n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8934acfc",
   "metadata": {},
   "source": [
    "## fedAvg algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "bc218b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Currently only allows for scrambling of labels as the corruption method\n",
    "\n",
    "def FedAvg(model, data, model_params, training_params, corrupt=False, cp=0.5):\n",
    "    model_dict = {1: NeuralNetwork, 2: LogisticRegression, 3: \"Net\"}\n",
    "    model = model_dict[model]\n",
    "    \n",
    "    num_clients = training_params[\"num_clients\"]\n",
    "    epochs = training_params[\"epochs\"]\n",
    "    batch_size = training_params[\"batch_size\"]\n",
    "    lr = training_params[\"lr\"]\n",
    "    \n",
    "    \n",
    "    if data == \"MNIST\":\n",
    "        # Load MNIST dataset - Normalized (MEAN STD)\n",
    "        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "        train_dataset = datasets.MNIST('../data', train=True, download=True, transform=transform)\n",
    "        if model == \"Net\":\n",
    "            model = Net_MNIST\n",
    "    \n",
    "    if data == \"CIFAR10\":\n",
    "        transform = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "            ])\n",
    "        train_dataset = datasets.CIFAR10('../data', train=True, download=True, transform=transform)\n",
    "        if model == \"Net\":\n",
    "            model = Net_CIFAR10\n",
    "\n",
    "    # Split data among clients\n",
    "    client_datasets = torch.utils.data.random_split(train_dataset, [len(train_dataset) // num_clients] * num_clients)\n",
    "    \n",
    "    # Initialize global model\n",
    "    central_model = model(model_params)\n",
    "\n",
    "    # Train global model using federated averaging\n",
    "    central_optimizer = optim.SGD(central_model.parameters(), lr=lr)\n",
    "    central_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        central_model.train()\n",
    "\n",
    "        # Train local models on each client\n",
    "        local_models = []\n",
    "        for client_dataset in client_datasets:\n",
    "            local_model = model(model_params)\n",
    "            local_model.load_state_dict(central_model.state_dict())\n",
    "            local_optimizer = optim.SGD(local_model.parameters(), lr=lr, momentum=0.9)\n",
    "            local_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "            for local_epoch in range(epochs):\n",
    "                local_model.train()\n",
    "\n",
    "                for local_data, local_target in torch.utils.data.DataLoader(client_dataset, batch_size=batch_size, shuffle=True):\n",
    "                    if corrupt:\n",
    "                        local_target = torch.as_tensor(data_corruption(3, local_data, local_target.tolist(), cp)).type(torch.LongTensor)\n",
    "                    local_optimizer.zero_grad()\n",
    "                    if model == Net_MNIST or model == Net_CIFAR10:\n",
    "                        local_output = local_model(local_data)\n",
    "                    else:\n",
    "                        local_output = local_model(local_data.view(local_data.shape[0], -1))\n",
    "                        \n",
    "                    local_loss = local_criterion(local_output, local_target)\n",
    "                    local_loss.backward()\n",
    "                    local_optimizer.step()\n",
    "\n",
    "            local_models.append(local_model)\n",
    "\n",
    "        # Update central model using federated averaging\n",
    "        for name, param in central_model.named_parameters():\n",
    "            if name.endswith('.bias'):\n",
    "                continue\n",
    "\n",
    "            local_params = torch.stack([local_model.state_dict()[name] for local_model in local_models])\n",
    "            central_mean = local_params.mean(0)\n",
    "            param.data = central_mean.data\n",
    "\n",
    "        central_loss = 0\n",
    "        central_accuracy = 0\n",
    "        central_optimizer.zero_grad()\n",
    "\n",
    "        for central_data, central_target in torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True):\n",
    "            if model == Net_MNIST or model == Net_CIFAR10:\n",
    "                central_output = central_model(central_data)\n",
    "            else:\n",
    "                central_output = central_model(central_data.view(central_data.shape[0], -1))\n",
    "            central_loss += central_criterion(central_output, central_target)\n",
    "            central_accuracy += (central_output.argmax(1) == central_target).float().sum()\n",
    "\n",
    "        central_loss /= len(train_dataset)\n",
    "        central_accuracy /= len(train_dataset)\n",
    "\n",
    "        central_loss.backward()\n",
    "        central_optimizer.step()\n",
    "\n",
    "        print(f'Epoch {epoch+1} - Global Training Loss: {central_loss:.4f}, Global Training Accuracy: {central_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6494c19",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fe4dab",
   "metadata": {},
   "source": [
    "### NN on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ae4e8324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Global Training Loss: 0.0696, Global Training Accuracy: 0.3762\n",
      "Epoch 2 - Global Training Loss: 0.0696, Global Training Accuracy: 0.4639\n"
     ]
    }
   ],
   "source": [
    "training_params = {\"epochs\": 2, \"lr\": 0.01, \"batch_size\": 32, \"num_clients\": 4}\n",
    "model_params = {\"input_dim\": 784, \"hidden_dim\": 128, \"output_dim\": 10}\n",
    "FedAvg(1, \"MNIST\", model_params, training_params, corrupt=True, cp=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f463510",
   "metadata": {},
   "source": [
    "### Logistic Regression on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "eb83de86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Global Loss: 0.0694, Global Accuracy: 0.2085\n",
      "Epoch 2 - Global Loss: 0.0683, Global Accuracy: 0.2913\n"
     ]
    }
   ],
   "source": [
    "training_params = {\"epochs\": 2, \"lr\": 0.01, \"batch_size\": 32, \"num_clients\": 4}\n",
    "model_params = {\"input_dim\": 728, \"output_dim\": 10}\n",
    "FedAvg(2, \"MNIST\", model_params, training_params, corrupt=True, cp=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e89452",
   "metadata": {},
   "source": [
    "### CNN on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "dfc0d87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Global Training Loss: 0.0718, Global Training Accuracy: 0.1016\n",
      "Epoch 2 - Global Training Loss: 0.0717, Global Training Accuracy: 0.1827\n"
     ]
    }
   ],
   "source": [
    "training_params = {\"epochs\": 2, \"lr\": 0.01, \"batch_size\": 32, \"num_clients\": 4}\n",
    "model_params = {\"in_channels\": 1}\n",
    "FedAvg(3, \"MNIST\", model_params, training_params, corrupt=True, cp=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3054af",
   "metadata": {},
   "source": [
    "### Logistic regression on CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a062651f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Epoch 1 - Global Training Loss: 0.0729, Global Training Accuracy: 0.0916\n",
      "Epoch 2 - Global Training Loss: 0.0717, Global Training Accuracy: 0.1251\n"
     ]
    }
   ],
   "source": [
    "training_params = {\"epochs\": 2, \"lr\": 0.01, \"batch_size\": 32, \"num_clients\": 4}\n",
    "model_params = {\"input_dim\": 3072, \"output_dim\": 10}\n",
    "FedAvg(2, \"CIFAR10\", model_params, training_params, corrupt=True, cp=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61be34de",
   "metadata": {},
   "source": [
    "### NN on CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9dce67c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Epoch 1 - Global Training Loss: 0.0716, Global Training Accuracy: 0.1374\n",
      "Epoch 2 - Global Training Loss: 0.0716, Global Training Accuracy: 0.1451\n"
     ]
    }
   ],
   "source": [
    "training_params = {\"epochs\": 2, \"lr\": 0.01, \"batch_size\": 32, \"num_clients\": 4}\n",
    "model_params = {\"input_dim\": 3072, \"hidden_dim\": 256, \"output_dim\": 10}\n",
    "FedAvg(1, \"CIFAR10\", model_params, training_params, corrupt=True, cp=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "8ae55c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Epoch 1 - Global Training Loss: 0.0527, Global Training Accuracy: 0.4062\n",
      "Epoch 2 - Global Training Loss: 0.0496, Global Training Accuracy: 0.4424\n",
      "Epoch 3 - Global Training Loss: 0.0476, Global Training Accuracy: 0.4638\n",
      "Epoch 4 - Global Training Loss: 0.0464, Global Training Accuracy: 0.4793\n",
      "Epoch 5 - Global Training Loss: 0.0453, Global Training Accuracy: 0.4917\n",
      "Epoch 6 - Global Training Loss: 0.0444, Global Training Accuracy: 0.4991\n",
      "Epoch 7 - Global Training Loss: 0.0439, Global Training Accuracy: 0.5055\n",
      "Epoch 8 - Global Training Loss: 0.0435, Global Training Accuracy: 0.5061\n"
     ]
    }
   ],
   "source": [
    "training_params = {\"epochs\": 8, \"lr\": 0.01, \"batch_size\": 32, \"num_clients\": 4}\n",
    "model_params = {\"input_dim\": 3072, \"hidden_dim\": 256, \"output_dim\": 10}\n",
    "FedAvg(1, \"CIFAR10\", model_params, training_params, corrupt=False, cp=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba41d669",
   "metadata": {},
   "source": [
    "### CNN on CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "8e0d2218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Epoch 1 - Global Training Loss: 0.0300, Global Training Accuracy: 0.2935\n",
      "Epoch 2 - Global Training Loss: 0.0337, Global Training Accuracy: 0.2567\n",
      "Epoch 3 - Global Training Loss: 0.0330, Global Training Accuracy: 0.2974\n",
      "Epoch 4 - Global Training Loss: 0.0310, Global Training Accuracy: 0.3210\n",
      "Epoch 5 - Global Training Loss: 0.0276, Global Training Accuracy: 0.3964\n",
      "Epoch 6 - Global Training Loss: 0.0247, Global Training Accuracy: 0.4512\n",
      "Epoch 7 - Global Training Loss: 0.0247, Global Training Accuracy: 0.4620\n",
      "Epoch 8 - Global Training Loss: 0.0255, Global Training Accuracy: 0.4565\n",
      "Epoch 9 - Global Training Loss: 0.0226, Global Training Accuracy: 0.5046\n",
      "Epoch 10 - Global Training Loss: 0.0215, Global Training Accuracy: 0.5321\n"
     ]
    }
   ],
   "source": [
    "training_params = {\"epochs\": 10, \"lr\": 0.01, \"batch_size\": 64, \"num_clients\": 4}\n",
    "model_params = {}\n",
    "FedAvg(3, \"CIFAR10\", model_params, training_params, corrupt=False, cp=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fbae02",
   "metadata": {},
   "source": [
    "#### Simple training loop to test the CNN w/o federated learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "38e79ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "[Epoch 1] loss: 2.093 | accuracy: 21.180\n",
      "[Epoch 2] loss: 2.192 | accuracy: 16.078\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train(model, trainloader, criterion, optimizer, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "        epoch_loss = running_loss / len(trainloader)\n",
    "        epoch_acc = 100 * correct / total\n",
    "        print('[Epoch %d] loss: %.3f | accuracy: %.3f' % (epoch + 1, epoch_loss, epoch_acc))\n",
    "    \n",
    "    print('Finished Training')\n",
    "model_params = {}\n",
    "# Load the CIFAR10 dataset\n",
    "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "trainloader = DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "# Define the model, loss function, and optimizer\n",
    "net = Net_CIFAR10(model_params)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Train the model\n",
    "train(net, trainloader, criterion, optimizer, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7cd359",
   "metadata": {},
   "source": [
    "### Random Forest - Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "93c4431d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9705\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Load the MNIST dataset\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=False)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor(), download=False)\n",
    "\n",
    "# Extract the features and labels from the dataset\n",
    "X_train = train_dataset.data.view(len(train_dataset), -1).numpy()\n",
    "y_train = train_dataset.targets.numpy()\n",
    "X_test = test_dataset.data.view(len(test_dataset), -1).numpy()\n",
    "y_test = test_dataset.targets.numpy()\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the classifier on the test set\n",
    "accuracy = rf.score(X_test, y_test)\n",
    "\n",
    "print(f\"Test accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12bd23e",
   "metadata": {},
   "source": [
    "# HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40bb5977",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nedan\\anaconda3\\envs\\ML_project\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading builder script: 100%|██████████| 6.21k/6.21k [00:00<00:00, 3.11MB/s]\n",
      "Downloading metadata: 100%|██████████| 5.56k/5.56k [00:00<00:00, 5.57MB/s]\n",
      "Downloading readme: 100%|██████████| 10.3k/10.3k [00:00<00:00, 3.46MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset food101/default to C:/Users/Nedan/.cache/huggingface/datasets/food101/default/0.0.0/7cebe41a80fb2da3f08fcbef769c8874073a86346f7fb96dc0847d4dfc318295...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 5.00G/5.00G [01:55<00:00, 43.4MB/s]\n",
      "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Downloading data:   0%|          | 0.00/1.47M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:   4%|▍         | 60.4k/1.47M [00:00<00:04, 310kB/s]\u001b[A\n",
      "Downloading data:  14%|█▎        | 200k/1.47M [00:00<00:01, 762kB/s] \u001b[A\n",
      "Downloading data:  31%|███▏      | 461k/1.47M [00:00<00:00, 1.45MB/s]\u001b[A\n",
      "Downloading data:  47%|████▋     | 687k/1.47M [00:00<00:00, 1.73MB/s]\u001b[A\n",
      "Downloading data: 100%|██████████| 1.47M/1.47M [00:00<00:00, 2.41MB/s]\u001b[A\n",
      "Downloading data files:  50%|█████     | 1/2 [00:01<00:01,  1.60s/it]\n",
      "Downloading data:   0%|          | 0.00/489k [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:   7%|▋         | 34.8k/489k [00:00<00:01, 347kB/s]\u001b[A\n",
      "Downloading data:  32%|███▏      | 159k/489k [00:00<00:00, 556kB/s] \u001b[A\n",
      "Downloading data: 100%|██████████| 489k/489k [00:00<00:00, 1.21MB/s]\u001b[A\n",
      "Downloading data files: 100%|██████████| 2/2 [00:02<00:00,  1.48s/it]\n",
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset food101 downloaded and prepared to C:/Users/Nedan/.cache/huggingface/datasets/food101/default/0.0.0/7cebe41a80fb2da3f08fcbef769c8874073a86346f7fb96dc0847d4dfc318295. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "food = load_dataset(\"food101\", split=\"train[:5000]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1618db0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a824679b84045dbb6f56ae6b36ea696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e13c9afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "food = food.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6aed233c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = food[\"train\"].features[\"label\"].names\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = str(i)\n",
    "    id2label[str(i)] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c0e2bf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'prime_rib'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label[str(79)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12b0caaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)rocessor_config.json: 100%|██████████| 160/160 [00:00<00:00, 26.7kB/s]\n",
      "C:\\Users\\Nedan\\anaconda3\\envs\\ML_project\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Nedan\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 502/502 [00:00<00:00, 101kB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "\n",
    "checkpoint = \"google/vit-base-patch16-224-in21k\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4e29781",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import RandomResizedCrop, Compose, Normalize, ToTensor\n",
    "\n",
    "normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
    "size = (\n",
    "    image_processor.size[\"shortest_edge\"]\n",
    "    if \"shortest_edge\" in image_processor.size\n",
    "    else (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
    ")\n",
    "_transforms = Compose([RandomResizedCrop(size), ToTensor(), normalize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c4b3357",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transforms(examples):\n",
    "    examples[\"pixel_values\"] = [_transforms(img.convert(\"RGB\")) for img in examples[\"image\"]]\n",
    "    del examples[\"image\"]\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65086a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "food = food.with_transform(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54205ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "\n",
    "data_collator = DefaultDataCollator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01ea5f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 4.20k/4.20k [00:00<00:00, 2.11MB/s]\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff0cd4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "abed88fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading pytorch_model.bin: 100%|██████████| 346M/346M [00:05<00:00, 66.7MB/s] \n",
      "Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTForImageClassification: ['pooler.dense.weight', 'pooler.dense.bias']\n",
      "- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForImageClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    num_labels=len(labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2a719c43",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Token is required (`token=True`), but no token found. You need to provide a token or be logged in to Hugging Face with `huggingface-cli login` or `huggingface_hub.login`. See https://huggingface.co/settings/tokens.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 18\u001b[0m\n\u001b[0;32m      1\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[0;32m      2\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy_awesome_food_model\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m     remove_unused_columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m     push_to_hub\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     16\u001b[0m )\n\u001b[1;32m---> 18\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfood\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfood\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML_project\\lib\\site-packages\\transformers\\trainer.py:550\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[1;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[0;32m    548\u001b[0m \u001b[38;5;66;03m# Create clone of distant repo and output directory if needed\u001b[39;00m\n\u001b[0;32m    549\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpush_to_hub:\n\u001b[1;32m--> 550\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_git_repo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mat_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    551\u001b[0m     \u001b[38;5;66;03m# In case of pull, we need to make sure every process has the latest.\u001b[39;00m\n\u001b[0;32m    552\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_tpu_available():\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML_project\\lib\\site-packages\\transformers\\trainer.py:3492\u001b[0m, in \u001b[0;36mTrainer.init_git_repo\u001b[1;34m(self, at_init)\u001b[0m\n\u001b[0;32m   3490\u001b[0m     repo_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mhub_model_id\n\u001b[0;32m   3491\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m repo_name:\n\u001b[1;32m-> 3492\u001b[0m     repo_name \u001b[38;5;241m=\u001b[39m \u001b[43mget_full_repo_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhub_token\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3494\u001b[0m \u001b[38;5;66;03m# Make sure the repo exists.\u001b[39;00m\n\u001b[0;32m   3495\u001b[0m create_repo(repo_name, token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mhub_token, private\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mhub_private_repo, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML_project\\lib\\site-packages\\transformers\\utils\\hub.py:795\u001b[0m, in \u001b[0;36mget_full_repo_name\u001b[1;34m(model_id, organization, token)\u001b[0m\n\u001b[0;32m    793\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_full_repo_name\u001b[39m(model_id: \u001b[38;5;28mstr\u001b[39m, organization: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, token: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    794\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 795\u001b[0m         username \u001b[38;5;241m=\u001b[39m \u001b[43mwhoami\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    796\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00musername\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    797\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML_project\\lib\\site-packages\\huggingface_hub\\hf_api.py:827\u001b[0m, in \u001b[0;36mHfApi.whoami\u001b[1;34m(self, token)\u001b[0m\n\u001b[0;32m    816\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwhoami\u001b[39m(\u001b[38;5;28mself\u001b[39m, token: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict:\n\u001b[0;32m    817\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;124;03m    Call HF API to know \"whoami\".\u001b[39;00m\n\u001b[0;32m    819\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    823\u001b[0m \u001b[38;5;124;03m            not provided.\u001b[39;00m\n\u001b[0;32m    824\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    825\u001b[0m     r \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m    826\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/api/whoami-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m--> 827\u001b[0m         headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_hf_headers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    828\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# If `token` is provided and not `None`, it will be used by default.\u001b[39;49;00m\n\u001b[0;32m    829\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# Otherwise, the token must be retrieved from cache or env variable.\u001b[39;49;00m\n\u001b[0;32m    830\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    831\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    832\u001b[0m     )\n\u001b[0;32m    833\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    834\u001b[0m         hf_raise_for_status(r)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML_project\\lib\\site-packages\\huggingface_hub\\hf_api.py:4205\u001b[0m, in \u001b[0;36mHfApi._build_hf_headers\u001b[1;34m(self, token, is_write_action, library_name, library_version, user_agent)\u001b[0m\n\u001b[0;32m   4202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4203\u001b[0m     \u001b[38;5;66;03m# Cannot do `token = token or self.token` as token can be `False`.\u001b[39;00m\n\u001b[0;32m   4204\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken\n\u001b[1;32m-> 4205\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuild_hf_headers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4207\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_write_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_write_action\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4208\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlibrary_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4209\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlibrary_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_version\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlibrary_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4210\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4211\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML_project\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:120\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    118\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML_project\\lib\\site-packages\\huggingface_hub\\utils\\_headers.py:117\u001b[0m, in \u001b[0;36mbuild_hf_headers\u001b[1;34m(token, is_write_action, library_name, library_version, user_agent)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;124;03mBuild headers dictionary to send in a HF Hub call.\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;124;03m        If `token=True` but token is not saved locally.\u001b[39;00m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;66;03m# Get auth token to send\u001b[39;00m\n\u001b[1;32m--> 117\u001b[0m token_to_send \u001b[38;5;241m=\u001b[39m \u001b[43mget_token_to_send\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m _validate_token_to_send(token_to_send, is_write_action\u001b[38;5;241m=\u001b[39mis_write_action)\n\u001b[0;32m    120\u001b[0m \u001b[38;5;66;03m# Combine headers\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML_project\\lib\\site-packages\\huggingface_hub\\utils\\_headers.py:149\u001b[0m, in \u001b[0;36mget_token_to_send\u001b[1;34m(token)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cached_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 149\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    150\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mToken is required (`token=True`), but no token found. You\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    151\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m need to provide a token or be logged in to Hugging Face with\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    152\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `huggingface-cli login` or `huggingface_hub.login`. See\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    153\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m https://huggingface.co/settings/tokens.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    154\u001b[0m         )\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cached_token\n\u001b[0;32m    157\u001b[0m \u001b[38;5;66;03m# Case implicit use of the token is forbidden by env variable\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: Token is required (`token=True`), but no token found. You need to provide a token or be logged in to Hugging Face with `huggingface-cli login` or `huggingface_hub.login`. See https://huggingface.co/settings/tokens."
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_food_model\",\n",
    "    remove_unused_columns=False,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=food[\"train\"],\n",
    "    eval_dataset=food[\"test\"],\n",
    "    tokenizer=image_processor,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
