{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "595d30c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (32x784 and 728x10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 184\u001b[0m\n\u001b[0;32m    182\u001b[0m training_params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m32\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_clients\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m4\u001b[39m}\n\u001b[0;32m    183\u001b[0m model_params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m728\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m10\u001b[39m}\n\u001b[1;32m--> 184\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mFedAvg\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMNIST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorrupt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLogistic_regression_MNIST_C4\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[0;32m    188\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(results, handle, protocol\u001b[38;5;241m=\u001b[39mpickle\u001b[38;5;241m.\u001b[39mHIGHEST_PROTOCOL)\n",
      "Cell \u001b[1;32mIn[1], line 141\u001b[0m, in \u001b[0;36mFedAvg\u001b[1;34m(model, data, model_params, training_params, corrupt, cp)\u001b[0m\n\u001b[0;32m    139\u001b[0m     local_output \u001b[38;5;241m=\u001b[39m local_model(local_data)\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 141\u001b[0m     local_output \u001b[38;5;241m=\u001b[39m \u001b[43mlocal_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m local_loss \u001b[38;5;241m=\u001b[39m local_criterion(local_output, local_target)\n\u001b[0;32m    144\u001b[0m local_loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML_project\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[1], line 74\u001b[0m, in \u001b[0;36mLogisticRegression.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 74\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML_project\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML_project\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32x784 and 728x10)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, model_params):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "        input_dim = model_params[\"input_dim\"]\n",
    "        hidden_dim = model_params[\"hidden_dim\"]\n",
    "        output_dim = model_params[\"output_dim\"]\n",
    "        \n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "class Net_MNIST(nn.Module):\n",
    "    def __init__(self, model_params):\n",
    "        super(Net_MNIST, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 5)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 5)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(32 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "            x = self.pool1(torch.relu(self.conv1(x)))\n",
    "            x = self.pool2(torch.relu(self.conv2(x)))\n",
    "            x = x.view(-1, 32 * 4 * 4)\n",
    "            x = torch.relu(self.fc1(x))\n",
    "            x = torch.relu(self.fc2(x))\n",
    "            x = self.fc3(x)\n",
    "            return x\n",
    "        \n",
    "class Net_CIFAR10(nn.Module):\n",
    "    def __init__(self, model_params):\n",
    "        super(Net_CIFAR10, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 5)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 5)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(32 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "            x = self.pool1(torch.relu(self.conv1(x)))\n",
    "            x = self.pool2(torch.relu(self.conv2(x)))\n",
    "            x = x.view(-1, 32 * 5 * 5)\n",
    "            x = torch.relu(self.fc1(x))\n",
    "            x = torch.relu(self.fc2(x))\n",
    "            x = self.fc3(x)\n",
    "            return x\n",
    "    \n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, model_params):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        \n",
    "        input_dim = model_params[\"input_dim\"]\n",
    "        output_dim = model_params[\"output_dim\"]\n",
    "        \n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "\n",
    "    \n",
    "# Currently only allows for scrambling of labels as the corruption method\n",
    "\n",
    "def FedAvg(model, data, model_params, training_params, corrupt=False, cp=0.5):\n",
    "    acc_dict = dict()\n",
    "    \n",
    "    model_dict = {1: NeuralNetwork, 2: LogisticRegression, 3: \"Net\"}\n",
    "    model = model_dict[model]\n",
    "    \n",
    "    num_clients = training_params[\"num_clients\"]\n",
    "    epochs = training_params[\"epochs\"]\n",
    "    batch_size = training_params[\"batch_size\"]\n",
    "    lr = training_params[\"lr\"]\n",
    "    \n",
    "    \n",
    "    if data == \"MNIST\":\n",
    "        # Load MNIST dataset - Normalized (MEAN STD)\n",
    "        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "        train_dataset = datasets.MNIST('../data', train=True, download=True, transform=transform)\n",
    "        if model == \"Net\":\n",
    "            model = Net_MNIST\n",
    "    \n",
    "    if data == \"CIFAR10\":\n",
    "        transform = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "            ])\n",
    "        train_dataset = datasets.CIFAR10('../data', train=True, download=True, transform=transform)\n",
    "        if model == \"Net\":\n",
    "            model = Net_CIFAR10\n",
    "\n",
    "    # Split data among clients\n",
    "    client_datasets = torch.utils.data.random_split(train_dataset, [len(train_dataset) // num_clients] * num_clients)\n",
    "    \n",
    "    # Initialize global model\n",
    "    central_model = model(model_params)\n",
    "\n",
    "    # Train global model using federated averaging\n",
    "    central_optimizer = optim.SGD(central_model.parameters(), lr=lr)\n",
    "    central_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        central_model.train()\n",
    "\n",
    "        # Train local models on each client\n",
    "        local_models = []\n",
    "        for client_dataset in client_datasets:\n",
    "            local_model = model(model_params)\n",
    "            local_model.load_state_dict(central_model.state_dict())\n",
    "            local_optimizer = optim.SGD(local_model.parameters(), lr=lr, momentum=0.9)\n",
    "            local_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "            for local_epoch in range(epochs):\n",
    "                local_model.train()\n",
    "\n",
    "                for local_data, local_target in torch.utils.data.DataLoader(client_dataset, batch_size=batch_size, shuffle=True):\n",
    "                    if corrupt:\n",
    "                        local_target = torch.as_tensor(data_corruption(3, local_data, local_target.tolist(), cp)).type(torch.LongTensor)\n",
    "                    local_optimizer.zero_grad()\n",
    "                    if model == Net_MNIST or model == Net_CIFAR10:\n",
    "                        local_output = local_model(local_data)\n",
    "                    else:\n",
    "                        local_output = local_model(local_data.view(local_data.shape[0], -1))\n",
    "                        \n",
    "                    local_loss = local_criterion(local_output, local_target)\n",
    "                    local_loss.backward()\n",
    "                    local_optimizer.step()\n",
    "\n",
    "            local_models.append(local_model)\n",
    "\n",
    "        # Update central model using federated averaging\n",
    "        for name, param in central_model.named_parameters():\n",
    "            if name.endswith('.bias'):\n",
    "                continue\n",
    "\n",
    "            local_params = torch.stack([local_model.state_dict()[name] for local_model in local_models])\n",
    "            central_mean = local_params.mean(0)\n",
    "            param.data = central_mean.data\n",
    "\n",
    "        central_loss = 0\n",
    "        central_accuracy = 0\n",
    "        central_optimizer.zero_grad()\n",
    "\n",
    "        for central_data, central_target in torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True):\n",
    "            if model == Net_MNIST or model == Net_CIFAR10:\n",
    "                central_output = central_model(central_data)\n",
    "            else:\n",
    "                central_output = central_model(central_data.view(central_data.shape[0], -1))\n",
    "            central_loss += central_criterion(central_output, central_target)\n",
    "            central_accuracy += (central_output.argmax(1) == central_target).float().sum()\n",
    "\n",
    "        central_loss /= len(train_dataset)\n",
    "        central_accuracy /= len(train_dataset)\n",
    "\n",
    "        central_loss.backward()\n",
    "        central_optimizer.step()\n",
    "        \n",
    "        acc_dict[epoch+1] = central_accuracy\n",
    "        print(f'Epoch {epoch+1} - Global Training Loss: {central_loss:.4f}, Global Training Accuracy: {central_accuracy:.4f}')\n",
    "    return acc_dict\n",
    "\n",
    "\n",
    "\n",
    "training_params = {\"epochs\": 10, \"lr\": 0.01, \"batch_size\": 32, \"num_clients\": 10}\n",
    "model_params = {\"input_dim\": 784, \"output_dim\": 10}\n",
    "results = FedAvg(2, \"MNIST\", model_params, training_params, corrupt=False, cp=0.9)\n",
    "\n",
    "\n",
    "with open('Logistic_regression_MNIST_C4', 'wb') as handle:\n",
    "    pickle.dump(results, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ca6d76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
