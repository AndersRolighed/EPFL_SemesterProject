{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b7ddd5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "methods_dict = {1:\"Missing_Data\", 2:\"Outliers\", 3:\"Labeling_Errors\", 4:\"Feature_Noise\"} \n",
    "def data_corruption(method, data, labels, corrupt_p):\n",
    "    corruption_method = methods_dict[method]\n",
    "    if corrupt_p <= 0 or corrupt_p >= 1:\n",
    "        print(\"Please choose a valid value for the corruption parameter (positive, above 0 and less than 1.)\")\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    # Go through each image, and create a randomized mask which sets pixels to a value of 0.\n",
    "    data_corrupted = []\n",
    "    if corruption_method == \"Missing_Data\":\n",
    "        for image in data:\n",
    "            mask = np.zeros(image.shape[0], dtype=int)\n",
    "            mask[int(image.shape[0]*corrupt_p):] = 1\n",
    "            random.shuffle(mask)\n",
    "            mask = mask.astype(bool)\n",
    "            corrupted_image = np.where(mask == False, 0, image)\n",
    "            data_corrupted.append(corrupted_image)\n",
    "        \n",
    "        return data_corrupted\n",
    "    \n",
    "    if corruption_method == \"Outliers\":\n",
    "        print(\"not yet implemented\")\n",
    "        \n",
    "    if corruption_method == \"Labeling_Errors\":\n",
    "        if len(labels) > 0:\n",
    "            label_names = np.unique(labels)\n",
    "            mask = np.zeros(len(labels), dtype=int)\n",
    "            mask[int(len(labels)*corrupt_p):] = 1\n",
    "            random.shuffle(mask)\n",
    "            mask = mask.astype(bool)\n",
    "            corrupted_labels = np.where(mask == False, \"needs_change\", labels)\n",
    "            for i in range(len(labels)):\n",
    "                if corrupted_labels[i] == \"needs_change\":\n",
    "                    options = np.delete(label_names, np.where(np.unique(labels) == labels[i])) ## Using np.unique to find the set of labels.\n",
    "                    corrupted_labels[i] = random.choice(options)\n",
    "\n",
    "            corrupted_labels = corrupted_labels.astype(type(labels[i]))\n",
    "            return corrupted_labels\n",
    "        \n",
    "        \n",
    "        else:\n",
    "            print(\"Please use an actual list for the labels\")\n",
    "        \n",
    "    if corruption_method == \"Feature_Noise\":\n",
    "        print(\"not yet implemented\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "530d84e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "corrupt = True\n",
    "\n",
    "def FedNB(cp):\n",
    "    accuracy_dict = dict()\n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # Load the MNIST dataset\n",
    "    train_dataset = MNIST(root='./data', train=True, transform=ToTensor(), download=True)\n",
    "    test_dataset = MNIST(root='./data', train=False, transform=ToTensor())\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "    # Convert the image data to numpy arrays\n",
    "    train_images = train_dataset.data.numpy().reshape(-1, 28*28)\n",
    "    train_labels = train_dataset.targets.numpy()\n",
    "    test_images = test_dataset.data.numpy().reshape(-1, 28*28)\n",
    "    test_labels = test_dataset.targets.numpy()\n",
    "\n",
    "    # Set the number of clients and local epochs\n",
    "    num_clients = 10\n",
    "    num_local_epochs = 3\n",
    "\n",
    "    # Define the global model\n",
    "    global_model = GaussianNB()\n",
    "\n",
    "    # Create a list to store local models\n",
    "    local_models = []\n",
    "\n",
    "    # Define the train_loaders for each client\n",
    "    train_loaders = []\n",
    "    train_dataset_size = len(train_dataset)\n",
    "    train_indices = list(range(train_dataset_size))\n",
    "    subset_size = train_dataset_size // num_clients\n",
    "\n",
    "    for client_idx in range(num_clients):\n",
    "        # Create a random subset of the training dataset for each client\n",
    "        subset_indices = train_indices[client_idx * subset_size: (client_idx + 1) * subset_size]\n",
    "        train_sampler = SubsetRandomSampler(subset_indices)\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            sampler=train_sampler,\n",
    "            num_workers=2,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        train_loaders.append(train_loader)\n",
    "\n",
    "    # Perform federated learning\n",
    "    for epoch in range(num_epochs):\n",
    "        # Perform local training for a few epochs\n",
    "        local_models = []\n",
    "\n",
    "        for client_idx in range(num_clients):\n",
    "            local_model = GaussianNB()\n",
    "\n",
    "            for local_epoch in range(num_local_epochs):\n",
    "                # Get the local training data\n",
    "                train_images, train_labels = next(iter(train_loaders[client_idx]))\n",
    "                train_images = train_images.view(train_images.size(0), -1)\n",
    "                if corrupt:\n",
    "                    train_labels = data_corruption(3, train_images, train_labels.numpy(), cp)\n",
    "                else:\n",
    "                    train_labels = train_labels.numpy()\n",
    "\n",
    "                # Fit the local model\n",
    "                local_model.fit(train_images, train_labels)\n",
    "\n",
    "            local_models.append(local_model)\n",
    "\n",
    "        # Aggregate the parameters from local models\n",
    "        global_model = GaussianNB()\n",
    "        global_model.fit(train_images, train_labels)\n",
    "        for key in global_model.get_params().keys():\n",
    "            param_values = [getattr(model, key) for model in local_models if getattr(model, key) is not None]\n",
    "            if param_values:\n",
    "                param_values = [float(param_value) for param_value in param_values]\n",
    "                mean_param_value = np.mean(param_values, axis=0)\n",
    "                setattr(global_model, key, mean_param_value)\n",
    "\n",
    "        # Evaluate the global model on the test set\n",
    "        global_predictions = global_model.predict(test_images)\n",
    "        accuracy = accuracy_score(test_labels, global_predictions)\n",
    "        print(f\"Epoch {epoch+1} - Global Model Accuracy: {accuracy}\")\n",
    "        accuracy_dict[epoch+1] = accuracy\n",
    "    return accuracy_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9adee64e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m corrupt_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0.95\u001b[39m, \u001b[38;5;241m0.9375\u001b[39m, \u001b[38;5;241m0.925\u001b[39m, \u001b[38;5;241m0.9125\u001b[39m, \u001b[38;5;241m0.90\u001b[39m, \u001b[38;5;241m0.875\u001b[39m, \u001b[38;5;241m0.85\u001b[39m, \u001b[38;5;241m0.65\u001b[39m, \u001b[38;5;241m0.50\u001b[39m, \u001b[38;5;241m0.35\u001b[39m, \u001b[38;5;241m0.20\u001b[39m, \u001b[38;5;241m0.05\u001b[39m]\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m corrupt_par \u001b[38;5;129;01min\u001b[39;00m corrupt_list:\n\u001b[1;32m----> 4\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mFedNB\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorrupt_par\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNB_MNIST_10_C5_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcorrupt_par\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_Labels\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[0;32m      7\u001b[0m         pickle\u001b[38;5;241m.\u001b[39mdump(results, handle, protocol\u001b[38;5;241m=\u001b[39mpickle\u001b[38;5;241m.\u001b[39mHIGHEST_PROTOCOL)\n",
      "Cell \u001b[1;32mIn[64], line 77\u001b[0m, in \u001b[0;36mFedNB\u001b[1;34m(cp)\u001b[0m\n\u001b[0;32m     73\u001b[0m local_model \u001b[38;5;241m=\u001b[39m GaussianNB()\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m local_epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_local_epochs):\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;66;03m# Get the local training data\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m     train_images, train_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loaders\u001b[49m\u001b[43m[\u001b[49m\u001b[43mclient_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     train_images \u001b[38;5;241m=\u001b[39m train_images\u001b[38;5;241m.\u001b[39mview(train_images\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m corrupt:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\EPFL_SemesterProject\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\EPFL_SemesterProject\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1329\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[0;32m   1328\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1329\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[0;32m   1332\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\EPFL_SemesterProject\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1295\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1291\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[0;32m   1292\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[0;32m   1293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1294\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1295\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1296\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m   1297\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\EPFL_SemesterProject\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1133\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[0;32m   1121\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[0;32m   1122\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[0;32m   1131\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1133\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[0;32m   1135\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1136\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[0;32m   1138\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\EPFL_SemesterProject\\lib\\multiprocessing\\queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[0;32m    112\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[1;32m--> 113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\EPFL_SemesterProject\\lib\\multiprocessing\\connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[1;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\EPFL_SemesterProject\\lib\\multiprocessing\\connection.py:330\u001b[0m, in \u001b[0;36mPipeConnection._poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_got_empty_message \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m    328\u001b[0m             _winapi\u001b[38;5;241m.\u001b[39mPeekNamedPipe(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 330\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\EPFL_SemesterProject\\lib\\multiprocessing\\connection.py:879\u001b[0m, in \u001b[0;36mwait\u001b[1;34m(object_list, timeout)\u001b[0m\n\u001b[0;32m    876\u001b[0m                 ready_objects\u001b[38;5;241m.\u001b[39madd(o)\n\u001b[0;32m    877\u001b[0m                 timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 879\u001b[0m     ready_handles \u001b[38;5;241m=\u001b[39m \u001b[43m_exhaustive_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaithandle_to_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m     \u001b[38;5;66;03m# request that overlapped reads stop\u001b[39;00m\n\u001b[0;32m    882\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ov \u001b[38;5;129;01min\u001b[39;00m ov_list:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\EPFL_SemesterProject\\lib\\multiprocessing\\connection.py:811\u001b[0m, in \u001b[0;36m_exhaustive_wait\u001b[1;34m(handles, timeout)\u001b[0m\n\u001b[0;32m    809\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    810\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m L:\n\u001b[1;32m--> 811\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_winapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWaitForMultipleObjects\u001b[49m\u001b[43m(\u001b[49m\u001b[43mL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    812\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;241m==\u001b[39m WAIT_TIMEOUT:\n\u001b[0;32m    813\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "corrupt_list = [0.95, 0.9375, 0.925, 0.9125, 0.90, 0.875, 0.85, 0.65, 0.50, 0.35, 0.20, 0.05]\n",
    "\n",
    "for corrupt_par in corrupt_list:\n",
    "    results = FedNB(corrupt_par)\n",
    "\n",
    "    with open(f'NB_MNIST_10_C5_{corrupt_par}_Labels', 'wb') as handle:\n",
    "        pickle.dump(results, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a004ee1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
