{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e95da159",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from random import randrange\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e5a849e",
   "metadata": {},
   "outputs": [],
   "source": [
    "methods_dict = {1:\"Missing_Data\", 2:\"Outliers\", 3:\"Labeling_Errors\", 4:\"Feature_Noise\"} \n",
    "def data_corruption(method, data, labels, corrupt_p):\n",
    "    corruption_method = methods_dict[method]\n",
    "    if corrupt_p <= 0 or corrupt_p >= 1:\n",
    "        print(\"Please choose a valid value for the corruption parameter (positive, above 0 and less than 1.)\")\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    # Go through each image, and create a randomized mask which sets pixels to a value of 0.\n",
    "    data_corrupted = []\n",
    "    if corruption_method == \"Missing_Data\":\n",
    "        for image in data:\n",
    "            mask = np.zeros(image.shape[0], dtype=int)\n",
    "            mask[int(image.shape[0]*corrupt_p):] = 1\n",
    "            random.shuffle(mask)\n",
    "            mask = mask.astype(bool)\n",
    "            corrupted_image = np.where(mask == False, 0, image)\n",
    "            data_corrupted.append(corrupted_image)\n",
    "        \n",
    "        return data_corrupted\n",
    "    \n",
    "    if corruption_method == \"Outliers\":\n",
    "        print(\"not yet implemented\")\n",
    "        \n",
    "    if corruption_method == \"Labeling_Errors\":\n",
    "        if len(labels) > 0:\n",
    "            label_names = np.unique(labels)\n",
    "            mask = np.zeros(len(labels), dtype=int)\n",
    "            mask[int(len(labels)*corrupt_p):] = 1\n",
    "            random.shuffle(mask)\n",
    "            mask = mask.astype(bool)\n",
    "            corrupted_labels = np.where(mask == False, \"needs_change\", labels)\n",
    "            for i in range(len(labels)):\n",
    "                if corrupted_labels[i] == \"needs_change\":\n",
    "                    options = np.delete(label_names, np.where(np.unique(labels) == labels[i])) ## Using np.unique to find the set of labels.\n",
    "                    corrupted_labels[i] = random.choice(options)\n",
    "\n",
    "            corrupted_labels = corrupted_labels.astype(type(labels[i]))\n",
    "            return corrupted_labels\n",
    "        \n",
    "        \n",
    "        else:\n",
    "            print(\"Please use an actual list for the labels\")\n",
    "        \n",
    "    if corruption_method == \"Feature_Noise\":\n",
    "        print(\"not yet implemented\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "051d146d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, model_params):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "        input_dim = model_params[\"input_dim\"]\n",
    "        hidden_dim = model_params[\"hidden_dim\"]\n",
    "        output_dim = model_params[\"output_dim\"]\n",
    "        \n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54887d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_MNIST(nn.Module):\n",
    "    def __init__(self, model_params):\n",
    "        super(Net_MNIST, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 5)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 5)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(32 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "            x = self.pool1(torch.relu(self.conv1(x)))\n",
    "            x = self.pool2(torch.relu(self.conv2(x)))\n",
    "            x = x.view(-1, 32 * 4 * 4)\n",
    "            x = torch.relu(self.fc1(x))\n",
    "            x = torch.relu(self.fc2(x))\n",
    "            x = self.fc3(x)\n",
    "            return x\n",
    "        \n",
    "        \n",
    "class Net_CIFAR10(nn.Module):\n",
    "    def __init__(self, model_params):\n",
    "        super(Net_CIFAR10, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 5)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 5)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(32 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "            x = self.pool1(torch.relu(self.conv1(x)))\n",
    "            x = self.pool2(torch.relu(self.conv2(x)))\n",
    "            x = x.view(-1, 32 * 5 * 5)\n",
    "            x = torch.relu(self.fc1(x))\n",
    "            x = torch.relu(self.fc2(x))\n",
    "            x = self.fc3(x)\n",
    "            return x\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d907c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, model_params):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        \n",
    "        input_dim = model_params[\"input_dim\"]\n",
    "        output_dim = model_params[\"output_dim\"]\n",
    "        \n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7c17007",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Currently only allows for scrambling of labels as the corruption method\n",
    "\n",
    "def FedAvg(model, data, model_params, training_params, corrupt=False, cp=0.5):\n",
    "    acc_dict = dict()\n",
    "    \n",
    "    model_dict = {1: NeuralNetwork, 2: LogisticRegression, 3: \"Net\"}\n",
    "    model = model_dict[model]\n",
    "    \n",
    "    num_clients = training_params[\"num_clients\"]\n",
    "    epochs = training_params[\"epochs\"]\n",
    "    batch_size = training_params[\"batch_size\"]\n",
    "    lr = training_params[\"lr\"]\n",
    "    \n",
    "    \n",
    "    if data == \"MNIST\":\n",
    "        # Load MNIST dataset - Normalized (MEAN STD)\n",
    "        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "        train_dataset = datasets.MNIST('../data', train=True, download=True, transform=transform)\n",
    "        test_dataset = datasets.MNIST('../data', train=False, download=True, transform=transform)\n",
    "        if model == \"Net\":\n",
    "            model = Net_MNIST\n",
    "    \n",
    "    if data == \"CIFAR10\":\n",
    "        transform = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "            ])\n",
    "        train_dataset = datasets.CIFAR10('../data', train=True, download=True, transform=transform)\n",
    "        test_dataset = datasets.CIFAR10('../data', train=False, download=True, transform=transform)\n",
    "        if model == \"Net\":\n",
    "            model = Net_CIFAR10\n",
    "\n",
    "    # Split data among clients\n",
    "    client_datasets = torch.utils.data.random_split(train_dataset, [len(train_dataset) // num_clients] * num_clients)\n",
    "    \n",
    "    # Initialize global model\n",
    "    central_model = model(model_params)\n",
    "\n",
    "    # Train global model using federated averaging\n",
    "    central_optimizer = optim.SGD(central_model.parameters(), lr=lr)\n",
    "    central_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        central_model.train()\n",
    "\n",
    "        # Train local models on each client\n",
    "        local_models = []\n",
    "        for client_dataset in client_datasets:\n",
    "            local_model = model(model_params)\n",
    "            local_model.load_state_dict(central_model.state_dict())\n",
    "            local_optimizer = optim.SGD(local_model.parameters(), lr=lr, momentum=0.9)\n",
    "            local_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "            for local_epoch in range(epochs):\n",
    "                local_model.train()\n",
    "\n",
    "                for local_data, local_target in torch.utils.data.DataLoader(client_dataset, batch_size=batch_size, shuffle=True):\n",
    "                    if corrupt:\n",
    "                        local_target = torch.as_tensor(data_corruption(3, local_data, local_target.tolist(), cp)).type(torch.LongTensor)\n",
    "                    local_optimizer.zero_grad()\n",
    "                    if model == Net_MNIST or model == Net_CIFAR10:\n",
    "                        local_output = local_model(local_data)\n",
    "                    else:\n",
    "                        local_output = local_model(local_data.view(local_data.shape[0], -1))\n",
    "                        \n",
    "                    local_loss = local_criterion(local_output, local_target)\n",
    "                    local_loss.backward()\n",
    "                    local_optimizer.step()\n",
    "\n",
    "            local_models.append(local_model)\n",
    "\n",
    "        # Update central model using federated averaging\n",
    "        for name, param in central_model.named_parameters():\n",
    "            if name.endswith('.bias'):\n",
    "                continue\n",
    "\n",
    "            local_params = torch.stack([local_model.state_dict()[name] for local_model in local_models])\n",
    "            central_mean = local_params.mean(0)\n",
    "            param.data = central_mean.data\n",
    "\n",
    "        central_loss = 0\n",
    "        central_accuracy = 0\n",
    "        central_optimizer.zero_grad()\n",
    "\n",
    "        for central_data, central_target in torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True):\n",
    "            if model == Net_MNIST or model == Net_CIFAR10:\n",
    "                central_output = central_model(central_data)\n",
    "            else:\n",
    "                central_output = central_model(central_data.view(central_data.shape[0], -1))\n",
    "            central_loss += central_criterion(central_output, central_target)\n",
    "            central_accuracy += (central_output.argmax(1) == central_target).float().sum()\n",
    "\n",
    "        central_loss /= len(test_dataset)\n",
    "        central_accuracy /= len(test_dataset)\n",
    "\n",
    "        central_loss.backward()\n",
    "        central_optimizer.step()\n",
    "        \n",
    "        acc_dict[epoch+1] = central_accuracy\n",
    "        print(f'Epoch {epoch+1} - Global Test Loss: {central_loss:.4f}, Global Test Accuracy: {central_accuracy:.4f}')\n",
    "    return acc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6537f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_params = {\"epochs\": 10, \"lr\": 0.01, \"batch_size\": 32, \"num_clients\": 5}\n",
    "model_params = {\"input_dim\": 3072, \"hidden_dim\": 256, \"output_dim\": 10}\n",
    "corrupt_list = [0.95, 0.9375, 0.925, 0.9125, 0.90, 0.875, 0.85, 0.80, 0.65, 0.50, 0.35, 0.20, 0.05]\n",
    "\n",
    "for corrupt_par in corrupt_list:\n",
    "    results = FedAvg(1, \"CIFAR10\", model_params, training_params, corrupt=True, cp=corrupt_par)\n",
    "\n",
    "    with open(f'NN_CIFAR10_10_C5_{corrupt_par}_Labels', 'wb') as handle:\n",
    "        pickle.dump(results, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
