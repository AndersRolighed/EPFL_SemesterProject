{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e95da159",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from random import randrange\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e5a849e",
   "metadata": {},
   "outputs": [],
   "source": [
    "methods_dict = {1:\"Missing_Data\", 2:\"Outliers\", 3:\"Labeling_Errors\", 4:\"Feature_Noise\"} \n",
    "def data_corruption(method, data, labels, corrupt_p):\n",
    "    corruption_method = methods_dict[method]\n",
    "    if corrupt_p <= 0 or corrupt_p >= 1:\n",
    "        print(\"Please choose a valid value for the corruption parameter (positive, above 0 and less than 1.)\")\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    # Go through each image, and create a randomized mask which sets pixels to a value of 0.\n",
    "    data_corrupted = []\n",
    "    if corruption_method == \"Missing_Data\":\n",
    "        for image in data:\n",
    "            mask = np.zeros(image.shape[1], dtype=int)\n",
    "            mask[int(image.shape[0]*corrupt_p):] = 1\n",
    "            random.shuffle(mask)\n",
    "            mask = mask.astype(bool)\n",
    "            corrupted_image1 = np.where(mask == False, 0, image[0])\n",
    "            corrupted_image2 = np.where(mask == False, 0, image[1])\n",
    "            corrupted_image3 = np.where(mask == False, 0, image[2])\n",
    "            corrupted_image = [corrupted_image1, corrupted_image2, corrupted_image3]\n",
    "            data_corrupted.append(corrupted_image)\n",
    "        \n",
    "        return data_corrupted\n",
    "    \n",
    "    if corruption_method == \"Outliers\":\n",
    "        print(\"not yet implemented\")\n",
    "        \n",
    "    if corruption_method == \"Labeling_Errors\":\n",
    "        if len(labels) > 0:\n",
    "            label_names = np.unique(labels)\n",
    "            mask = np.zeros(len(labels), dtype=int)\n",
    "            mask[int(len(labels)*corrupt_p):] = 1\n",
    "            random.shuffle(mask)\n",
    "            mask = mask.astype(bool)\n",
    "            corrupted_labels = np.where(mask == False, \"needs_change\", labels)\n",
    "            for i in range(len(labels)):\n",
    "                if corrupted_labels[i] == \"needs_change\":\n",
    "                    options = np.delete(label_names, np.where(np.unique(labels) == labels[i])) ## Using np.unique to find the set of labels.\n",
    "                    corrupted_labels[i] = random.choice(options)\n",
    "\n",
    "            corrupted_labels = corrupted_labels.astype(type(labels[i]))\n",
    "            return corrupted_labels\n",
    "        \n",
    "        \n",
    "        else:\n",
    "            print(\"Please use an actual list for the labels\")\n",
    "        \n",
    "    if corruption_method == \"Feature_Noise\":\n",
    "        print(\"not yet implemented\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "051d146d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, model_params):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "        input_dim = model_params[\"input_dim\"]\n",
    "        hidden_dim = model_params[\"hidden_dim\"]\n",
    "        output_dim = model_params[\"output_dim\"]\n",
    "        \n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54887d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_MNIST(nn.Module):\n",
    "    def __init__(self, model_params):\n",
    "        super(Net_MNIST, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 5)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 5)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(32 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "            x = self.pool1(torch.relu(self.conv1(x)))\n",
    "            x = self.pool2(torch.relu(self.conv2(x)))\n",
    "            x = x.view(-1, 32 * 4 * 4)\n",
    "            x = torch.relu(self.fc1(x))\n",
    "            x = torch.relu(self.fc2(x))\n",
    "            x = self.fc3(x)\n",
    "            return x\n",
    "        \n",
    "        \n",
    "class Net_CIFAR10(nn.Module):\n",
    "    def __init__(self, model_params):\n",
    "        super(Net_CIFAR10, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 5)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 5)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(32 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "            x = self.pool1(torch.relu(self.conv1(x)))\n",
    "            x = self.pool2(torch.relu(self.conv2(x)))\n",
    "            x = x.view(-1, 32 * 5 * 5)\n",
    "            x = torch.relu(self.fc1(x))\n",
    "            x = torch.relu(self.fc2(x))\n",
    "            x = self.fc3(x)\n",
    "            return x\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d907c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, model_params):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        \n",
    "        input_dim = model_params[\"input_dim\"]\n",
    "        output_dim = model_params[\"output_dim\"]\n",
    "        \n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7c17007",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Currently only allows for scrambling of labels as the corruption method\n",
    "\n",
    "def FedAvg(model, data, model_params, training_params, corrupt=False, cp=0.5):\n",
    "    acc_dict = dict()\n",
    "    \n",
    "    model_dict = {1: NeuralNetwork, 2: LogisticRegression, 3: \"Net\"}\n",
    "    model = model_dict[model]\n",
    "    \n",
    "    num_clients = training_params[\"num_clients\"]\n",
    "    epochs = training_params[\"epochs\"]\n",
    "    batch_size = training_params[\"batch_size\"]\n",
    "    lr = training_params[\"lr\"]\n",
    "    \n",
    "    \n",
    "    if data == \"MNIST\":\n",
    "        # Load MNIST dataset - Normalized (MEAN STD)\n",
    "        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "        train_dataset = datasets.MNIST('../data', train=True, download=True, transform=transform)\n",
    "        test_dataset = datasets.MNIST('../data', train=False, download=True, transform=transform)\n",
    "        if model == \"Net\":\n",
    "            model = Net_MNIST\n",
    "    \n",
    "    if data == \"CIFAR10\":\n",
    "        transform = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "            ])\n",
    "        train_dataset = datasets.CIFAR10('../data', train=True, download=True, transform=transform)\n",
    "        test_dataset = datasets.CIFAR10('../data', train=False, download=True, transform=transform)\n",
    "        if model == \"Net\":\n",
    "            model = Net_CIFAR10\n",
    "\n",
    "    # Split data among clients\n",
    "    client_datasets = torch.utils.data.random_split(train_dataset, [len(train_dataset) // num_clients] * num_clients)\n",
    "    \n",
    "    # Initialize global model\n",
    "    central_model = model(model_params)\n",
    "\n",
    "    # Train global model using federated averaging\n",
    "    central_optimizer = optim.SGD(central_model.parameters(), lr=lr)\n",
    "    central_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        central_model.train()\n",
    "\n",
    "        # Train local models on each client\n",
    "        local_models = []\n",
    "        for client_dataset in client_datasets:\n",
    "            local_model = model(model_params)\n",
    "            local_model.load_state_dict(central_model.state_dict())\n",
    "            local_optimizer = optim.SGD(local_model.parameters(), lr=lr, momentum=0.9)\n",
    "            local_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "            for local_epoch in range(epochs):\n",
    "                local_model.train()\n",
    "\n",
    "                for local_data, local_target in torch.utils.data.DataLoader(client_dataset, batch_size=batch_size, shuffle=True):\n",
    "                    if corrupt:\n",
    "                        local_data = torch.as_tensor(data_corruption(1, local_data, local_target.tolist(), cp)).type(torch.FloatTensor)\n",
    "                    local_optimizer.zero_grad()\n",
    "                    if model == Net_MNIST or model == Net_CIFAR10:\n",
    "                        local_output = local_model(local_data)\n",
    "                    else:\n",
    "                        local_output = local_model(local_data.view(local_data.shape[0], -1))\n",
    "                        \n",
    "                    local_loss = local_criterion(local_output, local_target)\n",
    "                    local_loss.backward()\n",
    "                    local_optimizer.step()\n",
    "\n",
    "            local_models.append(local_model)\n",
    "\n",
    "        # Update central model using federated averaging\n",
    "        for name, param in central_model.named_parameters():\n",
    "            if name.endswith('.bias'):\n",
    "                continue\n",
    "\n",
    "            local_params = torch.stack([local_model.state_dict()[name] for local_model in local_models])\n",
    "            central_mean = local_params.mean(0)\n",
    "            param.data = central_mean.data\n",
    "\n",
    "        central_loss = 0\n",
    "        central_accuracy = 0\n",
    "        central_optimizer.zero_grad()\n",
    "\n",
    "        for central_data, central_target in torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True):\n",
    "            if model == Net_MNIST or model == Net_CIFAR10:\n",
    "                central_output = central_model(central_data)\n",
    "            else:\n",
    "                central_output = central_model(central_data.view(central_data.shape[0], -1))\n",
    "            central_loss += central_criterion(central_output, central_target)\n",
    "            central_accuracy += (central_output.argmax(1) == central_target).float().sum()\n",
    "\n",
    "        central_loss /= len(test_dataset)\n",
    "        central_accuracy /= len(test_dataset)\n",
    "\n",
    "        central_loss.backward()\n",
    "        central_optimizer.step()\n",
    "        \n",
    "        acc_dict[epoch+1] = central_accuracy\n",
    "        print(f'Epoch {epoch+1} - Global Test Loss: {central_loss:.4f}, Global Test Accuracy: {central_accuracy:.4f}')\n",
    "    return acc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69cebcff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nedan\\AppData\\Local\\Temp\\ipykernel_10480\\1326981597.py:61: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:248.)\n",
      "  local_data = torch.as_tensor(data_corruption(1, local_data, local_target.tolist(), cp)).type(torch.FloatTensor)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m corrupt_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0.95\u001b[39m, \u001b[38;5;241m0.9375\u001b[39m, \u001b[38;5;241m0.925\u001b[39m, \u001b[38;5;241m0.9125\u001b[39m, \u001b[38;5;241m0.90\u001b[39m, \u001b[38;5;241m0.875\u001b[39m, \u001b[38;5;241m0.85\u001b[39m, \u001b[38;5;241m0.80\u001b[39m, \u001b[38;5;241m0.65\u001b[39m, \u001b[38;5;241m0.50\u001b[39m, \u001b[38;5;241m0.35\u001b[39m, \u001b[38;5;241m0.20\u001b[39m, \u001b[38;5;241m0.05\u001b[39m]\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m corrupt_par \u001b[38;5;129;01min\u001b[39;00m corrupt_list:\n\u001b[1;32m----> 7\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mFedAvg\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCIFAR10\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorrupt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorrupt_par\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCNN_CIFAR10_3_C5_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcorrupt_par\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_Missing\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[0;32m     10\u001b[0m         pickle\u001b[38;5;241m.\u001b[39mdump(results, handle, protocol\u001b[38;5;241m=\u001b[39mpickle\u001b[38;5;241m.\u001b[39mHIGHEST_PROTOCOL)\n",
      "Cell \u001b[1;32mIn[8], line 70\u001b[0m, in \u001b[0;36mFedAvg\u001b[1;34m(model, data, model_params, training_params, corrupt, cp)\u001b[0m\n\u001b[0;32m     68\u001b[0m             local_loss \u001b[38;5;241m=\u001b[39m local_criterion(local_output, local_target)\n\u001b[0;32m     69\u001b[0m             local_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 70\u001b[0m             \u001b[43mlocal_optimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m     local_models\u001b[38;5;241m.\u001b[39mappend(local_model)\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Update central model using federated averaging\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML_project\\lib\\site-packages\\torch\\optim\\optimizer.py:269\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m_ \u001b[38;5;241m=\u001b[39m args\n\u001b[0;32m    268\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m--> 269\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;66;03m# call optimizer step pre hooks\u001b[39;00m\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m pre_hook \u001b[38;5;129;01min\u001b[39;00m chain(_global_optimizer_pre_hooks\u001b[38;5;241m.\u001b[39mvalues(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_pre_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m    272\u001b[0m         result \u001b[38;5;241m=\u001b[39m pre_hook(\u001b[38;5;28mself\u001b[39m, args, kwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML_project\\lib\\site-packages\\torch\\autograd\\profiler.py:492\u001b[0m, in \u001b[0;36mrecord_function.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    491\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 492\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecord \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_record_function_enter_new\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML_project\\lib\\site-packages\\torch\\_ops.py:502\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    498\u001b[0m     \u001b[38;5;66;03m# overloading __call__ to ensure torch.ops.foo.bar()\u001b[39;00m\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;66;03m# is still callable from JIT\u001b[39;00m\n\u001b[0;32m    500\u001b[0m     \u001b[38;5;66;03m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;66;03m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[1;32m--> 502\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_op(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs \u001b[38;5;129;01mor\u001b[39;00m {})\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_params = {\"epochs\": 3, \"lr\": 0.01, \"batch_size\": 32, \"num_clients\": 5}\n",
    "model_params = {\"in_channels\": 3}\n",
    "\n",
    "corrupt_list = [0.95, 0.9375, 0.925, 0.9125, 0.90, 0.875, 0.85, 0.80, 0.65, 0.50, 0.35, 0.20, 0.05]\n",
    "\n",
    "for corrupt_par in corrupt_list:\n",
    "    results = FedAvg(3, \"CIFAR10\", model_params, training_params, corrupt=True, cp=corrupt_par)\n",
    "\n",
    "    with open(f'CNN_CIFAR10_3_C5_{corrupt_par}_Missing', 'wb') as handle:\n",
    "        pickle.dump(results, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
